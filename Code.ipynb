{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489a2705",
   "metadata": {},
   "outputs": [],
   "source": [
    "from geopy.distance import geodesic\n",
    "from sklearn import preprocessing\n",
    "import http.client, urllib.parse\n",
    "import matplotlib.pyplot as plt\n",
    "from bs4 import BeautifulSoup\n",
    "import lxml.html as lx\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import time\n",
    "import math\n",
    "import json\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6697d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Generates a sample of up to 500 houses from each city and writes them to csv\n",
    "Takes parameters csv_ and cities\n",
    "csv_ is the name of the file to be written\n",
    "cities is a list of tuples where the first value is the city name and the second value is the state code\n",
    "'''\n",
    "def getHouses(csv_, cities):\n",
    "\n",
    "    csvfile = open(csv_, 'w', newline='')\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=['address', 'price', 'city', 'squareFootage', 'latitude', 'longitude'])\n",
    "\n",
    "    url = \"https://realty-mole-property-api.p.rapidapi.com/saleListings\"\n",
    "\n",
    "    for city, state in cities:\n",
    "        querystring = {\"city\":city,\"state\":state,\"limit\":\"500\"}\n",
    "\n",
    "        headers = {\n",
    "            \"X-RapidAPI-Key\": \"43686bd243mshe8a0be6f9e0556cp10a1bbjsn58033bf0f546\",\n",
    "            \"X-RapidAPI-Host\": \"realty-mole-property-api.p.rapidapi.com\"\n",
    "        }\n",
    "\n",
    "        response = requests.request(\"GET\", url, headers=headers, params=querystring)\n",
    "\n",
    "        for house in response.json():\n",
    "            if type(house) == dict:\n",
    "                address = house.get('formattedAddress')\n",
    "                price = house.get('price')\n",
    "                city = house.get('city')\n",
    "                sqf = house.get('squareFootage')\n",
    "                latitude = house.get('latitude')\n",
    "                longitude = house.get('longitude')\n",
    "                writer.writerow({'address':address, 'price':price, 'city':city, 'squareFootage':sqf, \n",
    "                                 'latitude':latitude, 'longitude':longitude})\n",
    "\n",
    "    csvfile.close()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fda1ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#GET HOUSES SAMPLE\n",
    "cities = [('Los Angeles', 'CA'), ('Anaheim','CA'), ('Long Beach', 'CA'), ('Chicago', 'IL'), ('Naperville', 'IL'), ('Elgin', 'IL'),\n",
    "             ('Dallas', 'TX'), ('Fort Worth', 'TX'), ('Arlington', 'TX'), ('Washington', 'DC'), ('Arlington', 'VA'), ('Alexandria', 'VA')]\n",
    "\n",
    "getHouses('houses.csv', cities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da549121",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Returns a dataframe of places and their addresses\n",
    "Takes parameters cities and places\n",
    "cities is a list of tuples where the first value is the city name and the second value is the state code\n",
    "place is the place type\n",
    "'''\n",
    "\n",
    "def getPlaces(place, cities):\n",
    "    \n",
    "    sample = pd.DataFrame()\n",
    "    \n",
    "    for city, state in cities:    \n",
    "        addresses = list()\n",
    "        for i in range(1, 6):\n",
    "            if i == 1:\n",
    "                url = \"https://www.yellowpages.com/search?search_terms=\" + str(place) + \"&geo_location_terms=\" + str(city) + \"%2C+\" + str(state)\n",
    "            else:\n",
    "                url = \"https://www.yellowpages.com/search?search_terms=\" + str(place) + \"&geo_location_terms=\" + str(city) + \"%2C+\" + str(state) + \"&page=\" + str(i)\n",
    "            time.sleep(0.05)\n",
    "            response = requests.get(url,\n",
    "                                headers = {\"accept\" : \"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9\", \n",
    "                                            \"accept-encoding\": \"gzip, deflate, br\",\n",
    "                                            \"accept-language\": \"en-US,en;q=0.9\",\n",
    "                                            \"user-agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/107.0.0.0 Safari/537.36\"})\n",
    "\n",
    "            html = response.text\n",
    "            page = BeautifulSoup(html, \"html.parser\")\n",
    "            address = str(page.find_all(\"div\", class_=\"adr\")).split('<div class=\"adr\"><div class=\"street-address\">') \n",
    "\n",
    "            for j in range(1, len(address)):\n",
    "                if address[j] not in addresses:\n",
    "                    addresses.append(address[j].replace('</div><div class=\"locality\">', \",\").replace('</div></div>, ', \"\").replace(\"</div></div>]\", \"\").replace('<div class=\"adr\"><div class=\"locality\">', \"\"))\n",
    "                \n",
    "        #create columns address, city, and state\n",
    "                \n",
    "    return sample\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ca1ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Converts addresses to GPS coordinates and adds them to a dataframe\n",
    "Takes a df and returns the same df\n",
    "'''\n",
    "\n",
    "def getCoords(df):\n",
    "    \n",
    "    #api info\n",
    "    conn = http.client.HTTPConnection('api.positionstack.com')\n",
    "    key = 'df3b33e27a0f6451fd9aae993c6a26fa'\n",
    "    \n",
    "    \n",
    "    def query(coord, address):\n",
    "\n",
    "        params = urllib.parse.urlencode({\n",
    "            'access_key': key,\n",
    "            'query': address,\n",
    "            'limit': 1\n",
    "            })\n",
    "        \n",
    "        conn.request('GET', '/v1/forward?{}'.format(params))\n",
    "        results = conn.getresponse()\n",
    "        data = json.loads(results.read())['data'][0] #converts json to dict\n",
    "        \n",
    "        if coord == 'x':\n",
    "            result = data['latitude']\n",
    "        else:\n",
    "            result = data['longitude']\n",
    "            \n",
    "            return result\n",
    "    \n",
    "\n",
    "    lat = [query('x', address) for address in df.iloc[:,0]]\n",
    "    lon = [query('y', address) for address in df.iloc[:,0]]\n",
    "    \n",
    "    df['latitude'] = lat\n",
    "    df['longitude'] = lon\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "377c5cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Returns a list where each element is the distance, in miles, from a house to its nearest place\n",
    "houses is a df which contains Latitude and Longitude columns\n",
    "places is a df which contains Latitude and Longitude columns\n",
    "'''\n",
    "\n",
    "def getDist(houses, places):\n",
    "    distances = []\n",
    "        \n",
    "    for ind in houses.index:\n",
    "        y1 = houses['Longitude'][ind]\n",
    "        x1 = houses['Latitude'][ind]\n",
    "        coord1 = (y1,x1)\n",
    "        min_dist = 9999999\n",
    "        for i in places.index:\n",
    "            y2 = places['Latitude'][i]\n",
    "            x2 = places['Longitude'][i]\n",
    "            coord2 = (y2,x2)\n",
    "            try:\n",
    "                dist = geodesic(coord1, coord2).miles\n",
    "                if dist < min_dist:\n",
    "                    min_dist = dist\n",
    "            except:\n",
    "                pass\n",
    "        distances.append(min_dist)  \n",
    "        \n",
    "    return distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d6248b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPORT AND ORGANIZE DATA\n",
    "cities = [('Los Angeles', 'CA'), ('Anaheim','CA'), ('Long Beach', 'CA'), ('Chicago', 'IL'), ('Naperville', 'IL'), ('Elgin', 'IL'),\n",
    "             ('Dallas', 'TX'), ('Fort Worth', 'TX'), ('Arlington', 'TX'), ('Washington', 'DC'), ('Arlington', 'VA'), ('Alexandria', 'VA')]\n",
    "\n",
    "houses = pd.read_csv('../data/final/houses_clean.csv')\n",
    "\n",
    "houses_la = houses[houses['City'] == 'Los Angeles']\n",
    "houses_anaheim = houses[houses['City'] == 'Anaheim']\n",
    "houses_longbeach = houses[houses['City'] == 'Long Beach']\n",
    "\n",
    "houses_dallas = houses[houses['City'] == 'Dallas']\n",
    "houses_fortworth = houses[houses['City'] == 'Fort Worth']\n",
    "houses_atx = houses[houses['City'] == 'ArlingtonTX']\n",
    "\n",
    "houses_chicago = houses[houses['City'] == 'Chicago']\n",
    "houses_naperville = houses[houses['City'] == 'Naperville']\n",
    "houses_elgin = houses[houses['City'] == 'Elgin']\n",
    "\n",
    "houses_washington = houses[houses['City'] == 'Washington']\n",
    "houses_alexandria = houses[houses['City'] == 'Alexandria']\n",
    "houses_ava = houses[houses['City'] == 'ArlingtonVA']\n",
    "\n",
    "\n",
    "schools = pd.read_csv('../data/final/schools.csv')\n",
    "schools_la = schools[schools['City'] == 'Los Angeles']\n",
    "schools_anaheim = schools[schools['City'] == 'Anaheim']\n",
    "schools_longbeach = schools[schools['City'] == 'Long Beach']\n",
    "\n",
    "schools_dallas = schools[schools['City'] == 'Dallas']\n",
    "schools_fortworth = schools[schools['City'] == 'Fort Worth']\n",
    "schools_atx = schools[schools['City'] == 'ArlingtonTX']\n",
    "\n",
    "schools_chicago = schools[schools['City'] == 'Chicago']\n",
    "schools_naperville = schools[schools['City'] == 'Naperville']\n",
    "schools_elgin = schools[schools['City'] == 'Elgin']\n",
    "\n",
    "schools_washington = schools[schools['City'] == 'Washington']\n",
    "schools_alexandria = schools[schools['City'] == 'Alexandria']\n",
    "schools_ava = schools[schools['City'] == 'ArlingtonVA']\n",
    "\n",
    "\n",
    "hospitals = getPlaces('hospital', cities)\n",
    "\n",
    "hospitals_la = hospitals[hospitals['City'] == 'Los Angeles']\n",
    "hospitals_a = hospitals[hospitals['City'] == 'Anaheim']\n",
    "hospitals_lb = hospitals[hospitals['City'] == 'Long Beach']\n",
    "\n",
    "hospitals_d = hospitals[hospitals['City'] == 'Dallas']\n",
    "hospitals_fw = hospitals[hospitals['City'] == 'Fort Worth']\n",
    "hospitals_atx = hospitals[hospitals['City'] == 'Arlington' and hospitals['State'] == 'TX']\n",
    "\n",
    "hospitals_c = hospitals[hospitals['City'] == 'Chicago']\n",
    "hospitals_n = hospitals[hospitals['City'] == 'Naperville']\n",
    "hospitals_e = hospitals[hospitals['City'] == 'Elgin']\n",
    "\n",
    "hospitals_dc = hospitals[hospitals['City'] == 'Washington']\n",
    "hospitals_alex = hospitals[hospitals['City'] == 'Alexandria']\n",
    "hospitals_ava = hospitals[hospitals['City'] == 'Arlington' and hospitals['State'] == 'VA']\n",
    "\n",
    "grocery = getPlaces('grocery', cities)\n",
    "\n",
    "grocery_la = grocery[grocery['City'] == 'Los Angeles']\n",
    "grocery_a = grocery[grocery['City'] == 'Anaheim']\n",
    "grocery_lb = grocery[grocery['City'] == 'Long Beach']\n",
    "\n",
    "grocery_d = grocery[grocery['City'] == 'Dallas']\n",
    "grocery_fw = grocery[grocery['City'] == 'Fort Worth']\n",
    "grocery_atx = grocery[grocery['City'] == 'Arlington' and grocery['State'] == 'TX']\n",
    "\n",
    "grocery_c = grocery[grocery['City'] == 'Chicago']\n",
    "grocery_n = grocery[grocery['City'] == 'Naperville']\n",
    "grocery_e = grocery[grocery['City'] == 'Elgin']\n",
    "\n",
    "grocery_dc = grocery[grocery['City'] == 'Washington']\n",
    "grocery_alex = grocery[grocery['City'] == 'Alexandria']\n",
    "grocery_ava = grocery[grocery['City'] == 'Arlington' and grocery['State'] == 'VA']\n",
    "\n",
    "gym = getPlaces('gym', cities)\n",
    "\n",
    "gym_la = gym[gym['City'] == 'Los Angeles']\n",
    "gym_lb = gym[gym['City'] == 'Long Beach']\n",
    "gym_a = gym[gym['City'] == 'Anaheim']\n",
    "\n",
    "gym_d = gym[gym['City'] == 'Dallas']\n",
    "gym_atx = gym[gym['City'] == 'Arlington' and gym['State'] == 'TX']\n",
    "gym_fw = gym[gym['City'] == 'Fort Worth']\n",
    "\n",
    "gym_c = gym[gym['City'] == 'Chicago']\n",
    "gym_e = gym[gym['City'] == 'Elgin']\n",
    "gym_n = gym[gym['City'] == 'Naperville']\n",
    "\n",
    "gym_dc = gym[gym['City'] == 'Washington']\n",
    "gym_ava = gym[gym['City'] == 'Arlington' and gym['State'] == 'VA']\n",
    "gym_alex = gym[gym['City'] == 'Alexandria']\n",
    "\n",
    "parks = getPlaces('park', cities)\n",
    "\n",
    "parks_la = parks[parks['City'] == 'Los Angeles']\n",
    "parks_lb = parks[parks['City'] == 'Long Beach']\n",
    "parks_a = parks[parks['City'] == 'Anaheim']\n",
    "\n",
    "parks_d = parks[parks['City'] == 'Dallas']\n",
    "parks_atx = parks[parks['City'] == 'Arlington' and park['State'] == 'TX']\n",
    "parks_fw = parks[parks['City'] == 'Fort Worth']\n",
    "\n",
    "parks_c = parks[parks['City'] == 'Chicago']\n",
    "parks_e = parks[parks['City'] == 'Elgin']\n",
    "parks_n = parks[parks['City'] == 'Naperville']\n",
    "\n",
    "parks_dc = parks[parks['City'] == 'Washington']\n",
    "parks_ava = parks[parks['City'] == 'Arlington' and park['State'] == 'VA']\n",
    "parks_alex = parks[parks['City'] == 'Alexandria']\n",
    "\n",
    "beaches = getPlaces('beach', cities)\n",
    "\n",
    "beaches_la = beaches[beaches['City'] == 'Los Angeles']\n",
    "beaches_lb = beaches[beaches['City'] == 'Long Beach']\n",
    "beaches_a = beaches[beaches['City'] == 'Anaheim']\n",
    "\n",
    "beaches_d = beaches[beaches['City'] == 'Dallas']\n",
    "beaches_atx = beaches[beaches['City'] == 'Arlington' and beaches['State'] == 'TX']\n",
    "beaches_fw = beaches[beaches['City'] == 'Fort Worth']\n",
    "\n",
    "beaches_c = beaches[beaches['City'] == 'Chicago']\n",
    "beaches_e = beaches[beaches['City'] == 'Elgin']\n",
    "beaches_n = beaches[beaches['City'] == 'Naperville']\n",
    "\n",
    "beaches_dc = beaches[beaches['City'] == 'Washington']\n",
    "beaches_ava = beaches[beaches['City'] == 'Arlington' and beaches['State'] == 'VA']\n",
    "beaches_alex = beaches[beaches['City'] == 'Alexandria']\n",
    "\n",
    "cemetary = getPlaces('cemetery', cities)\n",
    "\n",
    "cemetary_la = cemetary[cemetary['City'] == 'Los Angeles']\n",
    "cemetary_lb = cemetary[cemetary['City'] == 'Long Beach']\n",
    "cemetary_a = cemetary[cemetary['City'] == 'Anaheim']\n",
    "\n",
    "cemetary_d = cemetary[cemetary['City'] == 'Dallas']\n",
    "cemetary_atx = cemetary[cemetary['City'] == 'ArlingtonTX']\n",
    "cemetary_fw = cemetary[cemetary['City'] == 'Fort Worth']\n",
    "\n",
    "cemetary_c = cemetary[cemetary['City'] == 'Chicago']\n",
    "cemetary_e = cemetary[cemetary['City'] == 'Elgin']\n",
    "cemetary_n = cemetary[cemetary['City'] == 'Naperville']\n",
    "\n",
    "cemetary_dc = cemetary[cemetary['City'] == 'Washington']\n",
    "cemetary_ava = cemetary[cemetary['City'] == 'ArlingtonVA']\n",
    "cemetary_alex = cemetary[cemetary['City'] == 'Alexandria']\n",
    "\n",
    "shopping = pd.read_csv('../data/final/shopping.csv')\n",
    "\n",
    "shopping_la = shopping[shopping['City'] == 'Los Angeles']\n",
    "shopping_lb = shopping[shopping['City'] == 'Long Beach']\n",
    "shopping_a = shopping[shopping['City'] == 'Anaheim']\n",
    "\n",
    "shopping_d = shopping[shopping['City'] == 'Dallas']\n",
    "shopping_atx = shopping[shopping['City'] == 'ArlingtonTX']\n",
    "shopping_fw = shopping[shopping['City'] == 'Fort Worth']\n",
    "\n",
    "shopping_c = shopping[shopping['City'] == 'Chicago']\n",
    "shopping_e = shopping[shopping['City'] == 'Elgin']\n",
    "shopping_n = shopping[shopping['City'] == 'Naperville']\n",
    "\n",
    "shopping_dc = shopping[shopping['City'] == 'Washington']\n",
    "shopping_ava = shopping[shopping['City'] == 'ArlingtonVA']\n",
    "shopping_alex = shopping[shopping['City'] == 'Alexandria']\n",
    "\n",
    "resturant = pd.read_csv('../data/final/restaurants.csv')\n",
    "\n",
    "resturant_la = resturant[resturant['City'] == 'Los Angeles']\n",
    "resturant_lb = resturant[resturant['City'] == 'Long Beach']\n",
    "resturant_a = resturant[resturant['City'] == 'Anaheim']\n",
    "\n",
    "resturant_d = resturant[resturant['City'] == 'Dallas']\n",
    "resturant_atx = resturant[resturant['City'] == 'ArlingtonTX']\n",
    "resturant_fw = resturant[resturant['City'] == 'Fort Worth']\n",
    "\n",
    "resturant_c = resturant[resturant['City'] == 'Chicago']\n",
    "resturant_e = resturant[resturant['City'] == 'Elgin']\n",
    "resturant_n = resturant[resturant['City'] == 'Naperville']\n",
    "\n",
    "resturant_dc = resturant[resturant['City'] == 'Washington']\n",
    "resturant_ava = resturant[resturant['City'] == 'ArlingtonVA']\n",
    "resturant_alex = resturant[resturant['City'] == 'Alexandria']\n",
    "\n",
    "golf = pd.read_csv('../data/final/golf.csv')\n",
    "\n",
    "golf_la = golf[golf['City'] == 'Los Angeles']\n",
    "golf_lb = golf[golf['City'] == 'Long Beach']\n",
    "golf_a = golf[golf['City'] == 'Anaheim']\n",
    "\n",
    "golf_d = golf[golf['City'] == 'Dallas']\n",
    "golf_atx = golf[golf['City'] == 'ArlingtonTX']\n",
    "golf_fw = golf[golf['City'] == 'Fort Worth']\n",
    "\n",
    "golf_c = golf[golf['City'] == 'Chicago']\n",
    "golf_e = golf[golf['City'] == 'Elgin']\n",
    "golf_n = golf[golf['City'] == 'Naperville']\n",
    "\n",
    "golf_dc = golf[golf['City'] == 'Washington']\n",
    "golf_ava = golf[golf['City'] == 'ArlingtonVA']\n",
    "golf_alex = golf[golf['City'] == 'Alexandria']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7a070b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "#EDA\n",
    "\n",
    "houses = pd.read_csv('../data/houses_clean.csv')\n",
    "\n",
    "houses_la = houses.loc[houses['City'] == 'Anaheim']\n",
    "houses_anaheim = houses.loc[houses['City'] == 'Los Angeles']\n",
    "houses_longbeach = houses.loc[houses['City'] == 'Long Beach']\n",
    "\n",
    "houses_dallas = houses.loc[houses['City'] == 'Dallas']\n",
    "houses_fortworth = houses.loc[houses['City'] == 'Fort Worth']\n",
    "houses_arlington_tx = houses.loc[houses['City'] == 'ArlingtonTX']\n",
    "\n",
    "houses_chicago = houses.loc[houses['City'] == 'Chicago']\n",
    "houses_naperville = houses.loc[houses['City'] == 'Naperville']\n",
    "houses_elgin = houses.loc[houses['City'] == 'Elgin']\n",
    "\n",
    "houses_washington = houses.loc[houses['City'] == 'Washington']\n",
    "houses_alexandria = houses.loc[houses['City'] == 'Alexandria']\n",
    "houses_arlington_va = houses.loc[houses['City'] == 'ArlingtonVA']\n",
    "\n",
    "#combine cities by state\n",
    "houses_ca = pd.concat([houses_la, houses_anaheim, houses_longbeach])\n",
    "houses_tx = pd.concat([houses_dallas, houses_fortworth, houses_arlington_tx])\n",
    "houses_dc = pd.concat([houses_washington, houses_alexandria, houses_arlington_va])\n",
    "houses_il = pd.concat([houses_chicago, houses_naperville, houses_elgin])\n",
    "\n",
    "#divide price by sq ft\n",
    "houses_ca['Housing Price/SQ Ft'] = houses_ca['Housing Price']/houses_ca['SQ Ft']\n",
    "houses_tx['Housing Price/SQ Ft'] = houses_tx['Housing Price']/houses_tx['SQ Ft']\n",
    "houses_dc['Housing Price/SQ Ft'] = houses_dc['Housing Price']/houses_dc['SQ Ft']\n",
    "houses_il['Housing Price/SQ Ft'] = houses_il['Housing Price']/houses_il['SQ Ft']\n",
    "\n",
    "#add column\n",
    "houses_ca['Housing Price/SQ Ft'] = pd.DataFrame(houses_ca['Housing Price/SQ Ft'])\n",
    "houses_tx['Housing Price/SQ Ft'] = pd.DataFrame(houses_tx['Housing Price/SQ Ft'])\n",
    "houses_dc['Housing Price/SQ Ft'] = pd.DataFrame(houses_dc['Housing Price/SQ Ft'])\n",
    "houses_il['Housing Price/SQ Ft'] = pd.DataFrame(houses_il['Housing Price/SQ Ft'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb506ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "houses_tx.sort_values(by = 'Housing Price/SQ Ft', ascending=True).head()\n",
    "houses_il.sort_values(by = 'Housing Price/SQ Ft', ascending=False).head()\n",
    "houses_ca.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b8ee1ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(houses_il['Housing Price/SQ Ft'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e3f63cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.boxplot(houses_il['Housing Price/SQ Ft'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf96898",
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalize\n",
    "houses_ca['Norm Price'] = preprocessing.MinMaxScaler().fit_transform(np.array(houses_ca['Housing Price/SQ Ft']).reshape(-1,1))\n",
    "houses_tx['Norm Price'] = preprocessing.MinMaxScaler().fit_transform(np.array(houses_tx['Housing Price/SQ Ft']).reshape(-1,1))\n",
    "houses_dc['Norm Price'] = preprocessing.MinMaxScaler().fit_transform(np.array(houses_dc['Housing Price/SQ Ft']).reshape(-1,1))\n",
    "houses_il['Norm Price'] = preprocessing.MinMaxScaler().fit_transform(np.array(houses_il['Housing Price/SQ Ft']).reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fea00131",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove outliers\n",
    "houses_ca = houses_ca.drop(houses_ca[houses_ca['Housing Price/SQ Ft'] > 3000].index)\n",
    "houses_dc = houses_dc.drop(houses_dc[houses_dc['Housing Price/SQ Ft'] > 3000].index)\n",
    "\n",
    "#subset so that all city dataframes have the normalized price values\n",
    "houses_la = houses_ca.loc[houses_ca['City'] == 'Anaheim']\n",
    "houses_anaheim = houses_ca.loc[houses_ca['City'] == 'Los Angeles']\n",
    "houses_longbeach = houses_ca.loc[houses_ca['City'] == 'Long Beach']\n",
    "\n",
    "houses_dallas = houses_tx.loc[houses_tx['City'] == 'Dallas']\n",
    "houses_fortworth = houses_tx.loc[houses_tx['City'] == 'Fort Worth']\n",
    "houses_arlington_tx = houses_tx.loc[houses_tx['City'] == 'ArlingtonTX']\n",
    "\n",
    "houses_chicago = houses_il.loc[houses_il['City'] == 'Chicago']\n",
    "houses_naperville = houses_il.loc[houses_il['City'] == 'Naperville']\n",
    "houses_elgin = houses_il.loc[houses_il['City'] == 'Elgin']\n",
    "\n",
    "houses_washington = houses_dc.loc[houses_dc['City'] == 'Washington']\n",
    "houses_alexandria = houses_dc.loc[houses_dc['City'] == 'Alexandria']\n",
    "houses_arlington_va = houses_dc.loc[houses_dc['City'] == 'ArlingtonVA']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "46df46aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LA DATA\n",
    "hospitals_ca = pd.concat([hospitals_la, hospitals_lb, hospitals_a])\n",
    "gym_ca = pd.concat([gym_la, gym_lb, gym_a])\n",
    "cemetary_ca = pd.concat([cemetary_la, cemetary_lb, cemetary_a])\n",
    "parks_ca = pd.concat([parks_la, parks_lb, parks_la])\n",
    "beaches_ca = pd.concat([beaches_la, beaches_lb, beaches_a])\n",
    "shopping_ca = pd.concat([shopping_la, shopping_lb, shopping_a])\n",
    "grocery_ca = pd.concat([grocery_la, grocery_lb, grocery_a])\n",
    "resturant_ca = pd.concat([resturant_la, resturant_lb, resturant_a])\n",
    "golf_ca = pd.concat([golf_la, golf_lb, golf_a])\n",
    "school_ca = pd.concat([schools_la, schools_longbeach, schools_anaheim])\n",
    "\n",
    "houses_ca['dist_hospitals'] = getDist(houses_ca, hospitals_ca)\n",
    "houses_ca['dist_gym'] = getDist(houses_ca, gym_ca)\n",
    "houses_ca['dist_cemetary'] = getDist(houses_ca, cemetary_ca)\n",
    "houses_ca['dist_parks'] = getDist(houses_ca, parks_ca)\n",
    "houses_ca['dist_beaches'] = getDist(houses_ca, beaches_ca)\n",
    "houses_ca['dist_shopping'] = getDist(houses_ca, shopping_ca)\n",
    "houses_ca['dist_grocery'] = getDist(houses_ca, grocery_ca)\n",
    "houses_ca['dist_resturant'] = getDist(houses_ca, resturant_ca)\n",
    "houses_ca['dist_golf'] = getDist(houses_ca, golf_ca)\n",
    "houses_ca['dist_school'] = getDist(houses_ca, school_ca)\n",
    "\n",
    "houses_ca = houses_ca.iloc[: , 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0bca4d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHICAGO DATA\n",
    "hospitals_il = pd.concat([hospitals_c, hospitals_e, hospitals_n])\n",
    "gym_il = pd.concat([gym_c, gym_e, gym_n])\n",
    "cemetary_il = pd.concat([cemetary_c, cemetary_e, cemetary_n])\n",
    "parks_il = pd.concat([parks_c, parks_e, parks_n])\n",
    "beaches_il = pd.concat([beaches_c, beaches_e, beaches_n])\n",
    "shopping_il = pd.concat([shopping_c, shopping_e, shopping_n])\n",
    "grocery_il = pd.concat([grocery_c, grocery_e, grocery_n])\n",
    "resturant_il = pd.concat([resturant_c, resturant_e, resturant_n])\n",
    "golf_il = pd.concat([golf_c, golf_e, golf_n])\n",
    "school_il = pd.concat([schools_chicago, schools_elgin, schools_naperville])\n",
    "\n",
    "\n",
    "houses_il['dist_hospitals'] = getDist(houses_il, hospitals_il)\n",
    "houses_il['dist_gym'] = getDist(houses_il, gym_il)\n",
    "houses_il['dist_cemetary'] = getDist(houses_il, cemetary_il)\n",
    "houses_il['dist_parks'] = getDist(houses_il, parks_il)\n",
    "houses_il['dist_beaches'] = getDist(houses_il, beaches_il)\n",
    "houses_il['dist_shopping'] = getDist(houses_il, shopping_il)\n",
    "houses_il['dist_grocery'] = getDist(houses_il, grocery_il)\n",
    "houses_il['dist_resturant'] = getDist(houses_il, resturant_il)\n",
    "houses_il['dist_golf'] = getDist(houses_il, golf_il)\n",
    "houses_il['dist_school'] = getDist(houses_il, school_il)\n",
    "\n",
    "houses_il = houses_il.iloc[: , 1:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bfada30b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DALLAS DATA\n",
    "hospitals_tx = pd.concat([hospitals_d, hospitals_fw, hospitals_atx])\n",
    "gym_tx = pd.concat([gym_d, gym_fw, gym_atx])\n",
    "cemetary_tx = pd.concat([cemetary_d, cemetary_fw, cemetary_atx])\n",
    "parks_tx = pd.concat([parks_d, parks_fw, parks_atx])\n",
    "beaches_tx = pd.concat([beaches_d, beaches_fw, beaches_atx])\n",
    "shopping_tx = pd.concat([shopping_d, shopping_fw, shopping_atx])\n",
    "grocery_tx = pd.concat([grocery_d, grocery_fw, grocery_atx])\n",
    "resturant_tx = pd.concat([resturant_d, resturant_fw, resturant_atx])\n",
    "golf_tx = pd.concat([golf_d, golf_fw, golf_atx])\n",
    "school_tx = pd.concat([schools_dallas, schools_fortworth, schools_atx])\n",
    "\n",
    "\n",
    "houses_tx['dist_hospitals'] = getDist(houses_tx, hospitals_tx)\n",
    "houses_tx['dist_gym'] = getDist(houses_tx, gym_tx)\n",
    "houses_tx['dist_cemetary'] = getDist(houses_tx, cemetary_tx)\n",
    "houses_tx['dist_parks'] = getDist(houses_tx, parks_tx)\n",
    "houses_tx['dist_beaches'] = getDist(houses_tx, beaches_tx)\n",
    "houses_tx['dist_shopping'] = getDist(houses_tx, shopping_tx)\n",
    "houses_tx['dist_grocery'] = getDist(houses_tx, grocery_tx)\n",
    "houses_tx['dist_resturant'] = getDist(houses_tx, resturant_tx)\n",
    "houses_tx['dist_golf'] = getDist(houses_tx, golf_tx)\n",
    "houses_tx['dist_school'] = getDist(houses_tx, school_tx)\n",
    "\n",
    "houses_tx = houses_tx.iloc[: , 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "414996ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DC DATA\n",
    "hospitals_dc = pd.concat([hospitals_dc, hospitals_ava, hospitals_alex])\n",
    "gym_dc = pd.concat([gym_dc, gym_ava, gym_alex])\n",
    "cemetary_dc = pd.concat([cemetary_dc, cemetary_ava, cemetary_alex])\n",
    "parks_dc = pd.concat([parks_dc, parks_ava, parks_alex])\n",
    "beaches_dc = pd.concat([beaches_dc, beaches_ava, beaches_alex])\n",
    "shopping_dca = pd.concat([shopping_dc, shopping_ava, shopping_alex])\n",
    "grocery_dc = pd.concat([grocery_dc, grocery_ava, grocery_alex])\n",
    "resturant_dc = pd.concat([resturant_dc, resturant_ava, resturant_alex])\n",
    "golf_dc = pd.concat([golf_dc, golf_ava, golf_alex])\n",
    "school_dc = pd.concat([schools_washington, schools_ava, schools_alex])\n",
    "\n",
    "\n",
    "houses_dc['dist_hospitals'] = getDist(houses_dc, hospitals_dc)\n",
    "houses_dc['dist_gym'] = getDist(houses_dc, gym_dc)\n",
    "houses_dc['dist_cemetary'] = getDist(houses_dc, cemetary_dc)\n",
    "houses_dc['dist_parks'] = getDist(houses_dc, parks_dc)\n",
    "houses_dc['dist_beaches'] = getDist(houses_dc, beaches_dc)\n",
    "houses_dc['dist_shopping'] = getDist(houses_dc, shopping_dc)\n",
    "houses_dc['dist_grocery'] = getDist(houses_dc, grocery_dc)\n",
    "houses_dc['dist_resturant'] = getDist(houses_dc, resturant_dc)\n",
    "houses_dc['dist_golf'] = getDist(houses_dc, golf_dc)\n",
    "houses_dc['dist_school'] = getDist(houses_dc, school_dc)\n",
    "\n",
    "houses_dc = houses_dc.iloc[: , 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "76d7bd9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Turned data to csv, put into final data\n",
    "\n",
    "#houses_ca.to_csv(\"mi_final_lametro.csv\")\n",
    "#houses_il.to_csv(\"mi_final_chimetro.csv\")\n",
    "#houses_tx.to_csv(\"mi_final_txmetro.csv\")\n",
    "#houses_dc.to_csv(\"mi_final_dcmetro.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "91c4300b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#EDA\n",
    "\n",
    "#transform\n",
    "np.seterr(divide = 'ignore') \n",
    "houses_ca = pd.read_csv('../data/final/metro/mi_final_lametro.csv')\n",
    "houses_ca['log'] = np.log(houses_ca['Norm Price'])\n",
    "houses_ca['sqrt'] = np.sqrt(houses_ca['Norm Price'])\n",
    "houses_ca['cube_root'] = np.power(houses_ca['Norm Price'], 1/3)\n",
    "\n",
    "houses_il = pd.read_csv('../data/final/metro/mi_final_chimetro.csv')\n",
    "houses_il['log'] = np.log(houses_il['Norm Price'])\n",
    "houses_il['sqrt'] = np.sqrt(houses_il['Norm Price'])\n",
    "houses_il['cube_root'] = np.power(houses_il['Norm Price'], 1/3)\n",
    "\n",
    "houses_tx = pd.read_csv('../data/final/metro/mi_final_txmetro.csv')\n",
    "houses_tx['log'] = np.log(houses_tx['Norm Price'])\n",
    "houses_tx['sqrt'] = np.sqrt(houses_tx['Norm Price'])\n",
    "houses_tx['cube_root'] = np.power(houses_tx['Norm Price'], 1/3)\n",
    "\n",
    "houses_dc = pd.read_csv('../data/final/metro/mi_final_dcmetro.csv')\n",
    "houses_dc['log'] = np.log(houses_dc['Norm Price'])\n",
    "houses_dc['sqrt'] = np.sqrt(houses_dc['Norm Price'])\n",
    "houses_dc['cube_root'] = np.power(houses_dc['Norm Price'], 1/3)\n",
    "\n",
    "houses_ca = houses_ca[['Address','City', 'Longitude','Latitude', 'SQ Ft', 'Housing Price','Housing Price/SQ Ft', 'Norm Price', 'log', 'sqrt', 'cube_root', 'dist_hospitals', 'dist_gym', 'dist_cemetary', 'dist_parks', 'dist_beaches', 'dist_shopping', 'dist_grocery', 'dist_resturant', 'dist_golf', 'dist_school']]\n",
    "houses_il = houses_il[['Address','City', 'Longitude','Latitude', 'SQ Ft', 'Housing Price','Housing Price/SQ Ft', 'Norm Price', 'log', 'sqrt', 'cube_root', 'dist_hospitals', 'dist_gym', 'dist_cemetary', 'dist_parks', 'dist_beaches', 'dist_shopping', 'dist_grocery', 'dist_resturant', 'dist_golf', 'dist_school']]\n",
    "houses_tx = houses_tx[['Address','City', 'Longitude','Latitude', 'SQ Ft', 'Housing Price','Housing Price/SQ Ft', 'Norm Price', 'log', 'sqrt', 'cube_root', 'dist_hospitals', 'dist_gym', 'dist_cemetary', 'dist_parks', 'dist_beaches', 'dist_shopping', 'dist_grocery', 'dist_resturant', 'dist_golf', 'dist_school']]\n",
    "houses_dc = houses_dc[['Address','City', 'Longitude','Latitude', 'SQ Ft', 'Housing Price','Housing Price/SQ Ft', 'Norm Price', 'log', 'sqrt', 'cube_root', 'dist_hospitals', 'dist_gym', 'dist_cemetary', 'dist_parks', 'dist_beaches', 'dist_shopping', 'dist_grocery', 'dist_resturant', 'dist_golf', 'dist_school']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f36af47",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(14, 14))\n",
    "sns.heatmap(houses_ca.corr(), annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "385ebbdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(14, 14))\n",
    "sns.heatmap(houses_il.corr(), annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e9feb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(14, 14))\n",
    "sns.heatmap(houses_tx.corr(), annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45618f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(14, 14))\n",
    "sns.heatmap(houses_dc.corr(), annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "667a716c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#separate to explore\n",
    "houses_ca_eda = houses_ca.iloc[:,19:]\n",
    "houses_il_eda = houses_il.iloc[:,19:]\n",
    "houses_tx_eda = houses_tx.iloc[:,19:]\n",
    "houses_dc_eda = houses_dc.iloc[:,19:]\n",
    "\n",
    "houses_ca_eda = pd.concat([houses_ca['Norm Price'], houses_ca_eda], axis = 1)\n",
    "houses_ca_eda = pd.concat([houses_ca['Housing Price'], houses_ca_eda], axis = 1)\n",
    "\n",
    "houses_il_eda = pd.concat([houses_il['Norm Price'], houses_il_eda], axis = 1)\n",
    "houses_il_eda = pd.concat([houses_il['Housing Price'], houses_il_eda], axis = 1)\n",
    "\n",
    "houses_tx_eda = pd.concat([houses_tx['Norm Price'], houses_tx_eda], axis = 1)\n",
    "houses_tx_eda = pd.concat([houses_tx['Housing Price'], houses_tx_eda], axis = 1)\n",
    "\n",
    "houses_dc_eda = pd.concat([houses_dc['Norm Price'], houses_dc_eda], axis = 1)\n",
    "houses_dc_eda = pd.concat([houses_dc['Housing Price'], houses_dc_eda], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd10853",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(houses_ca_eda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf7274f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(houses_il_eda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b60b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(houses_tx_eda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf8eaca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(houses_dc_eda)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a207257f",
   "metadata": {},
   "source": [
    "### **Regression Analysis**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df81043d",
   "metadata": {},
   "source": [
    "\n",
    "**Data Manipulation** \n",
    "\n",
    "Combining all metro datas into one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "443227bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Combining all METRO Data\n",
    " \n",
    "lametro = pd.read_csv(\"../data/final/metro/mi_final_lametro.csv\")\n",
    "chimetro = pd.read_csv(\"../data/final/metro/mi_final_chimetro.csv\")\n",
    "dcmetro = pd.read_csv(\"../data/final/metro/mi_final_dcmetro.csv\")\n",
    "dalmetro = pd.read_csv(\"../data/final/metro/mi_final_txmetro.csv\")\n",
    "\n",
    "lametro_adj = lametro[[\"Norm Price\",\"City\",\"dist_hospitals\",\"dist_gym\",\"dist_cemetary\",\"dist_parks\",\"dist_beaches\",\"dist_shopping\",\"dist_grocery\",\"dist_resturant\",\"dist_golf\",\"dist_school\"]]\n",
    "chimetro_adj = chimetro[[\"Norm Price\",\"City\",\"dist_hospitals\",\"dist_gym\",\"dist_cemetary\",\"dist_parks\",\"dist_beaches\",\"dist_shopping\",\"dist_grocery\",\"dist_resturant\",\"dist_golf\",\"dist_school\"]]\n",
    "dcmetro_adj = dcmetro[[\"Norm Price\",\"City\",\"dist_hospitals\",\"dist_gym\",\"dist_cemetary\",\"dist_parks\",\"dist_beaches\",\"dist_shopping\",\"dist_grocery\",\"dist_resturant\",\"dist_golf\",\"dist_school\"]]\n",
    "dalmetro_adj = dalmetro[[\"Norm Price\",\"City\",\"dist_hospitals\",\"dist_gym\",\"dist_cemetary\",\"dist_parks\",\"dist_beaches\",\"dist_shopping\",\"dist_grocery\",\"dist_resturant\",\"dist_golf\",\"dist_school\"]]\n",
    "\n",
    "lametro_adj = lametro_adj.rename(columns={lametro_adj.columns[0]: \"Norm\" })\n",
    "chimetro_adj = chimetro_adj.rename(columns={chimetro_adj.columns[0]: \"Norm\" })\n",
    "dcmetro_adj = dcmetro_adj.rename(columns={dcmetro_adj.columns[0]: \"Norm\" })\n",
    "dalmetro_adj = dalmetro_adj.rename(columns={dalmetro_adj.columns[0]: \"Norm\" })\n",
    "\n",
    "metro = pd.concat([lametro_adj,chimetro_adj,dcmetro_adj,dalmetro_adj])\n",
    "#metro.to_csv(\"metro.csv\")\n",
    "\n",
    "nummetro = metro.drop(columns = [\"City\",\"Norm\"])\n",
    "nummetro = nummetro.reset_index(drop=True)\n",
    "citymetro = metro[[\"City\",\"Norm\"]]\n",
    "citymetro = citymetro.reset_index(drop=True)\n",
    "nummetro = nummetro[(np.abs(stats.zscore(nummetro)) < 3).all(axis=1)]\n",
    "nummetro = nummetro.join(citymetro)\n",
    "nummetro = nummetro.drop(nummetro.index[nummetro['Norm'] <= 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fbbc226",
   "metadata": {},
   "source": [
    "**Model I**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd026dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "resulta = sm.OLS(nummetro['Norm'], nummetro.drop(columns=[\"Norm\",\"City\"])).fit() #Initialize the GLS \n",
    "print(resulta.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f0d8f49",
   "metadata": {},
   "source": [
    "Assumption Check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afdb7672",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multicollinearity: VIF\n",
    "from patsy import dmatrices\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "  \n",
    "y, X = dmatrices('Norm ~ dist_hospitals+dist_gym+dist_cemetary+dist_parks+dist_beaches+dist_shopping+dist_grocery+dist_resturant+dist_golf+dist_school', data=nummetro, return_type='dataframe')\n",
    "vif = pd.DataFrame()\n",
    "vif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
    "vif['variable'] = X.columns\n",
    "vif = vif.set_index(\"variable\")\n",
    "vif\n",
    "vif.style.applymap(lambda x: 'background-color : purple' if x>5 else '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f624ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residual Plots\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format ='retina'\n",
    "import statsmodels.stats.api as sms\n",
    "sns.set_style('darkgrid')\n",
    "sns.mpl.rcParams['figure.figsize'] = (15.0, 9.0)\n",
    "\n",
    "def linearity_test(model, y):\n",
    "    '''\n",
    "    Function for visually inspecting the assumption of linearity in a linear regression model.\n",
    "    It plots observed vs. predicted values and residuals vs. predicted values.\n",
    "    \n",
    "    Args:\n",
    "    * model - fitted OLS model from statsmodels\n",
    "    * y - observed values\n",
    "    '''\n",
    "    fitted_vals = model.predict()\n",
    "    resids = model.resid\n",
    "\n",
    "    fig, ax = plt.subplots(1,2)\n",
    "    \n",
    "    sns.regplot(x=fitted_vals, y=y, lowess=True, ax=ax[0], line_kws={'color': 'red'})\n",
    "    ax[0].set_title('Observed vs. Predicted Values', fontsize=16)\n",
    "    ax[0].set(xlabel='Predicted', ylabel='Observed')\n",
    "\n",
    "    sns.regplot(x=fitted_vals, y=resids, lowess=True, ax=ax[1], line_kws={'color': 'red'})\n",
    "    ax[1].set_title('Residuals vs. Predicted Values', fontsize=16)\n",
    "    ax[1].set(xlabel='Predicted', ylabel='Residuals')\n",
    "    \n",
    "linearity_test(resulta, y=nummetro[[\"Norm\"]])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379dd542",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unequal Variances Check: BP\n",
    "import statsmodels.stats.api as sms\n",
    "sms.het_breuschpagan(resulta.resid, resulta.model.exog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f35efcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normality: QQ Plot\n",
    "stats.probplot(resulta.resid, dist=\"norm\", plot= plt)\n",
    "plt.title(\"Model1 Residuals Q-Q Plot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dacc5351",
   "metadata": {},
   "source": [
    "**Model II: Reciprocal Transformation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0949430b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA MANIPULATION\n",
    "nummetro = metro.drop(columns = [\"City\",\"Norm\"])\n",
    "nummetro = np.exp(-nummetro)\n",
    "Q1 = nummetro.quantile(q=.25)\n",
    "Q3 = nummetro.quantile(q=.75)\n",
    "IQR = nummetro.apply(stats.iqr)\n",
    "nummetro = nummetro[~((nummetro < (Q1-1.5*IQR)) | (nummetro > (Q3+1.5*IQR))).any(axis=1)]\n",
    "nummetro = nummetro.reset_index(drop=True)\n",
    "citymetro = metro[[\"City\",\"Norm\"]]\n",
    "citymetro[\"Norm\"] = np.exp(-citymetro[\"Norm\"])\n",
    "citymetro = citymetro.reset_index(drop=True)\n",
    "\n",
    "nummetro = nummetro.join(citymetro)\n",
    "#nummetro.to_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df68963e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regression Analysis\n",
    "\n",
    "nummetro = nummetro[nummetro.replace([np.inf, -np.inf], np.nan).notnull().all(axis=1)] \n",
    "resulta = sm.OLS(nummetro['Norm'], nummetro.drop(columns=[\"Norm\",\"City\"])).fit() \n",
    "print(resulta.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad1b1b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unequal Variances: BP\n",
    "sms.het_breuschpagan(resulta.resid, resulta.model.exog)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4902e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multicollinearity: VIF\n",
    "from patsy import dmatrices\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "  \n",
    "y, X = dmatrices('Norm ~ dist_hospitals+dist_gym+dist_cemetary+dist_parks+dist_beaches+dist_shopping+dist_grocery+dist_resturant+dist_golf+dist_school', data=nummetro, return_type='dataframe')\n",
    "vif = pd.DataFrame()\n",
    "vif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
    "vif['variable'] = X.columns\n",
    "vif = vif.set_index(\"variable\")\n",
    "vif\n",
    "vif.style.applymap(lambda x: 'background-color : purple' if x>5 else '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3385809a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residual Plots\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format ='retina'\n",
    "sns.set_style('darkgrid')\n",
    "sns.mpl.rcParams['figure.figsize'] = (15.0, 9.0)\n",
    "\n",
    "linearity_test(resulta, y=nummetro[[\"Norm\"]])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e1bc83b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normality: QQ Plot\n",
    "stats.probplot(resulta.resid, dist=\"norm\", plot= plt)\n",
    "plt.title(\"Model1 Residuals Q-Q Plot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7cf8c17",
   "metadata": {},
   "source": [
    "**Initial Models: Separate Metro Areas**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3807910c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Los Angeles Metro Area\n",
    "citymetro = lametro_adj[[\"Norm\",\"City\"]]\n",
    "citymetro = citymetro.reset_index(drop=True)\n",
    "lametro_adj = lametro_adj.drop(columns = [\"City\",\"Norm\"])\n",
    "lametro_adj = lametro_adj.reset_index(drop=True)\n",
    "lametro_adj = lametro_adj[(np.abs(stats.zscore(lametro_adj)) < 3).all(axis=1)]\n",
    "lametro_adj = lametro_adj.join(citymetro)\n",
    "resultla = sm.OLS(lametro_adj['Norm'], lametro_adj.drop(columns=[\"Norm\",\"City\"])).fit() #Initialize the GLS \n",
    "print(resultla.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b69f1362",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DC Metro Area\n",
    "citymetro = dcmetro_adj[[\"Norm\",\"City\"]]\n",
    "citymetro = citymetro.reset_index(drop=True)\n",
    "dcmetro_adj = dcmetro_adj.drop(columns = [\"City\",\"Norm\"])\n",
    "dcmetro_adj = dcmetro_adj.reset_index(drop=True)\n",
    "dcmetro_adj = dcmetro_adj[(np.abs(stats.zscore(dcmetro_adj)) < 3).all(axis=1)]\n",
    "dcmetro_adj = dcmetro_adj.join(citymetro)\n",
    "resultdc = sm.OLS(dcmetro_adj['Norm'], dcmetro_adj.drop(columns=[\"Norm\",\"City\"])).fit() #Initialize the GLS \n",
    "print(resultdc.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f95cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dallas Metro Area\n",
    "citymetro = dalmetro_adj[[\"Norm\",\"City\"]]\n",
    "citymetro = citymetro.reset_index(drop=True)\n",
    "dalmetro_adj = dalmetro_adj.drop(columns = [\"City\",\"Norm\"])\n",
    "dalmetro_adj = dalmetro_adj.reset_index(drop=True)\n",
    "dalmetro_adj = dalmetro_adj[(np.abs(stats.zscore(dalmetro_adj)) < 3).all(axis=1)]\n",
    "dalmetro_adj = dalmetro_adj.join(citymetro)\n",
    "resultdal = sm.OLS(dalmetro_adj['Norm'], dalmetro_adj.drop(columns=[\"Norm\",\"City\"])).fit() #Initialize the GLS \n",
    "print(resultdal.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "347a75f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chicago Metro Area\n",
    "citymetro = chimetro_adj[[\"Norm\",\"City\"]]\n",
    "citymetro = citymetro.reset_index(drop=True)\n",
    "chimetro_adj = chimetro_adj.drop(columns = [\"City\",\"Norm\"])\n",
    "chimetro_adj = chimetro_adj.reset_index(drop=True)\n",
    "chimetro_adj = chimetro_adj[(np.abs(stats.zscore(chimetro_adj)) < 3).all(axis=1)]\n",
    "chimetro_adj = chimetro_adj.join(citymetro)\n",
    "resultchi = sm.OLS(chimetro_adj['Norm'], chimetro_adj.drop(columns=[\"Norm\",\"City\"])).fit() #Initialize the GLS \n",
    "print(resultchi.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ebf9e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VIF Check for All Four Metro Areas\n",
    "from patsy import dmatrices\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "  \n",
    "yla, Xla = dmatrices('Norm ~ dist_hospitals+dist_gym+dist_cemetary+dist_parks+dist_beaches+dist_shopping+dist_grocery+dist_resturant+dist_golf+dist_school', data=lametro_adj, return_type='dataframe')\n",
    "ydc, Xdc = dmatrices('Norm ~ dist_hospitals+dist_gym+dist_cemetary+dist_parks+dist_beaches+dist_shopping+dist_grocery+dist_resturant+dist_golf+dist_school', data=dcmetro_adj, return_type='dataframe')\n",
    "ydal, Xdal = dmatrices('Norm ~ dist_hospitals+dist_gym+dist_cemetary+dist_parks+dist_beaches+dist_shopping+dist_grocery+dist_resturant+dist_golf+dist_school', data=dalmetro_adj, return_type='dataframe')\n",
    "ychi, Xchi = dmatrices('Norm ~ dist_hospitals+dist_gym+dist_cemetary+dist_parks+dist_beaches+dist_shopping+dist_grocery+dist_resturant+dist_golf+dist_school', data=chimetro_adj, return_type='dataframe')\n",
    "vif = pd.DataFrame()\n",
    "vif['VIF LA'] = [variance_inflation_factor(Xla.values, i) for i in range(Xla.shape[1])]\n",
    "vif['VIF DC'] = [variance_inflation_factor(Xdc.values, i) for i in range(Xdc.shape[1])]\n",
    "vif['VIF Dal'] = [variance_inflation_factor(Xdal.values, i) for i in range(Xdal.shape[1])]\n",
    "vif['VIF CHI'] = [variance_inflation_factor(Xchi.values, i) for i in range(Xchi.shape[1])]\n",
    "vif['variable'] = Xla.columns\n",
    "vif = vif.set_index(\"variable\")\n",
    "vif\n",
    "vif.style.applymap(lambda x: 'background-color : purple' if x>5 else '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "482dc605",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNEQUAL VARIANCE for all Metro Areas\n",
    "import statsmodels.stats.api as sms\n",
    "la = sms.het_breuschpagan(resultla.resid, resultla.model.exog)\n",
    "dc = sms.het_breuschpagan(resultdc.resid, resultdc.model.exog)\n",
    "dal = sms.het_breuschpagan(resultdal.resid, resultdal.model.exog)\n",
    "chi = sms.het_breuschpagan(resultchi.resid, resultchi.model.exog)\n",
    "print(la,dc,dal,chi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7059a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NORMALITY for all Metro Areas\n",
    "la = sms.jarque_bera(resultla.resid)\n",
    "dc = sms.jarque_bera(resultdc.resid)\n",
    "dal = sms.jarque_bera(resultdal.resid)\n",
    "chi = sms.jarque_bera(resultchi.resid)\n",
    "print(la,dc,dal,chi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "360be73d",
   "metadata": {},
   "source": [
    "**Model II for separate metros**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e91a5db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Los Angeles\n",
    "nummetro = lametro_adj.drop(columns = [\"City\",\"Norm\"])\n",
    "nummetro = np.exp(-nummetro)\n",
    "Q1 = nummetro.quantile(q=.25)\n",
    "Q3 = nummetro.quantile(q=.75)\n",
    "IQR = nummetro.apply(stats.iqr)\n",
    "nummetro = nummetro[~((nummetro < (Q1-1.5*IQR)) | (nummetro > (Q3+1.5*IQR))).any(axis=1)]\n",
    "nummetro = nummetro.reset_index(drop=True)\n",
    "citymetro = lametro_adj[[\"City\",\"Norm\"]]\n",
    "citymetro[\"Norm\"] = np.exp(-citymetro[\"Norm\"])\n",
    "citymetro = citymetro.reset_index(drop=True)\n",
    "nummetro = nummetro.join(citymetro)\n",
    "resultla = sm.OLS(nummetro['Norm'], nummetro.drop(columns=[\"Norm\",\"City\"])).fit() #Initialize the GLS \n",
    "print(resultla.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "400b4f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DC\n",
    "nummetro = dcmetro_adj.drop(columns = [\"City\",\"Norm\"])\n",
    "nummetro = np.exp(-nummetro)\n",
    "Q1 = nummetro.quantile(q=.25)\n",
    "Q3 = nummetro.quantile(q=.75)\n",
    "IQR = nummetro.apply(stats.iqr)\n",
    "nummetro = nummetro[~((nummetro < (Q1-1.5*IQR)) | (nummetro > (Q3+1.5*IQR))).any(axis=1)]\n",
    "nummetro = nummetro.reset_index(drop=True)\n",
    "citymetro = dcmetro_adj[[\"City\",\"Norm\"]]\n",
    "citymetro[\"Norm\"] = np.exp(-citymetro[\"Norm\"])\n",
    "citymetro = citymetro.reset_index(drop=True)\n",
    "nummetro = nummetro.join(citymetro)\n",
    "resultdc = sm.OLS(nummetro['Norm'], nummetro.drop(columns=[\"Norm\",\"City\",\"dist_hospitals\",\"dist_school\"])).fit() #Initialize the GLS \n",
    "print(resultdc.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7826f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chicago\n",
    "nummetro = chimetro_adj.drop(columns = [\"City\",\"Norm\"])\n",
    "nummetro = np.exp(-nummetro)\n",
    "Q1 = nummetro.quantile(q=.25)\n",
    "Q3 = nummetro.quantile(q=.75)\n",
    "IQR = nummetro.apply(stats.iqr)\n",
    "nummetro = nummetro[~((nummetro < (Q1-1.5*IQR)) | (nummetro > (Q3+1.5*IQR))).any(axis=1)]\n",
    "nummetro = nummetro.reset_index(drop=True)\n",
    "citymetro = chimetro_adj[[\"City\",\"Norm\"]]\n",
    "citymetro[\"Norm\"] = np.exp(-citymetro[\"Norm\"])\n",
    "citymetro = citymetro.reset_index(drop=True)\n",
    "nummetro = nummetro.join(citymetro)\n",
    "resultchi = sm.OLS(nummetro['Norm'], nummetro.drop(columns=[\"Norm\",\"City\",\"dist_cemetary\",\"dist_beaches\",\"dist_shopping\",\"dist_grocery\"])).fit() #Initialize the GLS \n",
    "print(resultchi.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b8c98a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dallas\n",
    "nummetro = dalmetro_adj.drop(columns = [\"City\",\"Norm\"])\n",
    "#nummetro = np.exp(-nummetro)\n",
    "Q1 = nummetro.quantile(q=.25)\n",
    "Q3 = nummetro.quantile(q=.75)\n",
    "IQR = nummetro.apply(stats.iqr)\n",
    "nummetro = nummetro[~((nummetro < (Q1-1.5*IQR)) | (nummetro > (Q3+1.5*IQR))).any(axis=1)]\n",
    "nummetro = nummetro.reset_index(drop=True)\n",
    "citymetro = dalmetro_adj[[\"City\",\"Norm\"]]\n",
    "citymetro[\"Norm\"] = np.exp(-citymetro[\"Norm\"])\n",
    "citymetro = citymetro.reset_index(drop=True)\n",
    "nummetro = nummetro.join(citymetro)\n",
    "resultdal = sm.OLS(nummetro['Norm'], nummetro.drop(columns=[\"Norm\",\"City\"])).fit() #Initialize the GLS \n",
    "print(resultdal.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc184b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normality Check for LA & Dallas (not shown on summary)\n",
    "la = sms.jarque_bera(resultla.resid)\n",
    "dc = sms.jarque_bera(resultdc.resid)\n",
    "chi = sms.jarque_bera(resultchi.resid)\n",
    "dal = sms.jarque_bera(resultdal.resid)\n",
    "print(la,dc,chi,dal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121d2e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNEQUAL VARIANCE for all Metro Areas\n",
    "la = sms.het_breuschpagan(resultla.resid, resultla.model.exog)\n",
    "dc = sms.het_breuschpagan(resultdc.resid, resultdc.model.exog)\n",
    "dal = sms.het_breuschpagan(resultdal.resid, resultdal.model.exog)\n",
    "chi = sms.het_breuschpagan(resultchi.resid, resultchi.model.exog)\n",
    "print(la,dc,dal,chi)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0 (tags/v3.10.0:b494f59, Oct  4 2021, 19:00:18) [MSC v.1929 64 bit (AMD64)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "022267cb3ac0217d7f2a6d7dd62fe775715f5fa133ea9ab6d3329351e860f4c2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
